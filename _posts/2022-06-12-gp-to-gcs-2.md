---
title: "How to store all your photos and videos in Google Cloud Platform, for the rest of your life, at the lowest price and automated. (Part 2: Executing the solution)"
categories:
    - cloud
tags:
    - google
    - cloud
    - python
    - google cloud platform
    - google cloud storage
    - google cloud functions
    - google drive
    - google photos
---
<figure>
  <a href="/assets/img/2022-06-12/google.png"><img src="/assets/img/2022-06-12/google.png"></a>
  <figcaption></figcaption>
</figure>

<figure>
  <a href="/assets/img/2022-06-12/banner-google-cloud.png"><img src="/assets/img/2022-06-12/banner-google-cloud.png"></a>
  <figcaption></figcaption>
</figure>

After deciding to use Google Cloud Platform (GCP) as a cloud service provider to create a system that  allow us to automatically upload our photos to the cloud, as a cheaper alternative to using Google Drive and Google Photos directly, it's time to get hands on the code.
First we must have the Google Photos app installed on our mobile and activate the synchronization of photos with the cloud.
Let's then define how will be the interaction with the Google Photos API.

<figure>
  <a href="/assets/img/2022-06-12/diagram.png"><img src="/assets/img/2022-06-10/diagram.png"></a>
  <figcaption></figcaption>
</figure>

Using Google Cloud Platform, through a Cloud Scheduler alarm that will be executed once a month, we will execute a Google Cloud Function that will connect to the Google Photos API, using our GCP credentials, and then download all the images and videos from Google Photos the Google Cloud Platform since the last execution date of this function. All the information will be store in a bucket of type Archive.
To get started, you first need to create a project on GCP. To do this we go to the projects tab, create a new one and assign it a name.

<figure>
  <a href="/assets/img/2022-06-12/project1.png"><img src="/assets/img/2022-06-10/project1.png"></a>
  <figcaption></figcaption>
</figure>

<figure>
  <a href="/assets/img/2022-06-12/project2.png"><img src="/assets/img/2022-06-10/project2.png"></a>
  <figcaption></figcaption>
</figure>

Then it is necessary to activate the Google Photos API and create the credentials with which we will request access to our personal account.
Clarify that the authentication method to consume the Google Photos API is OAuth2 using the REST API. So with the created credentials we will request permission from the user to access Google Photos, and we will be assigned a token that we will use to be able to download the images and videos in the account. This token can become obsolete after a certain tperiod, but it can be updated. Authorization to access the account is requested only the first time we create the token. In all other cases, the token can be refreshed without authorization from the account holder.
To activate the API we go to the GCP APIs and services section, search for the Google Photos API, activate it and then click on Manage to access the internal options.

<figure>
  <a href="/assets/img/2022-06-12/api_gogle_photos.png"><img src="/assets/img/2022-06-10/api_gogle_photos.png"></a>
  <figcaption></figcaption>
</figure>

In the vertical panel on the left we select the credentials section and create new ones of the OAuth client ID type.

<figure>
  <a href="/assets/img/2022-06-12/api_gogle_photos2.png"><img src="/assets/img/2022-06-10/api_gogle_photos2.png"></a>
  <figcaption></figcaption>
</figure>

We then define the application type as Desktop App (for example), and give our credentials a name.

<figure>
  <a href="/assets/img/2022-06-12/api_gogle_photos3.png"><img src="/assets/img/2022-06-10/api_gogle_photos3.png"></a>
  <figcaption></figcaption>
</figure>

Later we download the file with the credentials in json format on our local computer.

<figure>
  <a href="/assets/img/2022-06-12/api_gogle_photos4.png"><img src="/assets/img/2022-06-10/api_gogle_photos4.png"></a>
  <figcaption></figcaption>
</figure>

With the credentials already downloaded, we can proceed to create the code that will request access to the Google Photo information. We will create the code with Python using several libraries that will make it easier for us to interact with the REST API and we will test it first in a local environment.
We will use the following Google.py module, which is a version of the one that can be found at the following link: https://learndataanalysis.org/google-py-file-source-code/ as support to easily interact with the REST API using Python.

```python
import pickle
import os
from google_auth_oauthlib.flow import Flow, InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.auth.transport.requests import Request

def Create_Service(client_secret_file, api_name, api_version, *scopes):
    print(client_secret_file, api_name, api_version, scopes, sep='-')
    CLIENT_SECRET_FILE = client_secret_file
    API_SERVICE_NAME = api_name
    API_VERSION = api_version
    SCOPES = [scope for scope in scopes[0]]
    print(SCOPES)
    cred = None
    pickle_file = f'token_{API_SERVICE_NAME}_{API_VERSION}.pickle'
    if os.path.exists(pickle_file):
        with open(pickle_file, 'rb') as token:
            cred = pickle.load(token)
    if not cred or not cred.valid:
        if cred and cred.expired and cred.refresh_token:
            cred.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)
            cred = flow.run_local_server()
        with open(pickle_file, 'wb') as token:
            pickle.dump(cred, token)
    try:
        service = build(API_SERVICE_NAME, API_VERSION, credentials=cred, static_discovery=False)
        print(API_SERVICE_NAME, 'service created successfully')
        return service
    except Exception as e:
        print('Unable to connect.')
        print(e)
        return None
```

In addition, we will create the main.py file with which we will make use of the Google.py module. In this main.py file we create the connection with the Google Photos API and print the name of the stored images or videos on the console. Remember before using the code, install the necessary libraries indicated in the first commented line.

```python
# !pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib

import os
import Google
import requests

client_secret_file = 'credentials.json'
api_name = 'photoslibrary'
api_version = 'v1'
scopes = ['https://www.googleapis.com/auth/photoslibrary']

service = Google.Create_Service(client_secret_file, api_name, api_version, scopes)
mymediaItems = service.mediaItems().list(pageSize=50).execute()

for mediaItem in mymediaItems['mediaItems']:
    print(mediaItem['filename'])
```

The first time we run the code, it will open a browser tab in which we will have to give access to the app with our Google account. And then a token is created in .pickle format that we will use to access the Google Photos account.
As long as the token exists, the program will not request permission to access, it will simply check if the token is valid, and if it is not, it will update it automatically.
By executing the main.py program and following all the authorization instructions, we should be able to see a list in the console with the different objects that are in the Google Photos account.

<figure>
  <a href="/assets/img/2022-06-12/console_results_local.png"><img src="/assets/img/2022-06-10/console_results_local.png"></a>
  <figcaption></figcaption>
</figure>

Now we will go to the GCP website and create two buckets, one will be our bucket of type Archive and another of type Standard. In the Archive we will store all the images and videos, while the Standard will be used to store the credentials and the authentication token to access the Google Photos API. This way we will not have to be generating a new one every time.
To do this, we go to Google Cloud Storage and create both buckets with the name that we like the most. Remember that one must be Standard and the other Archive. In Europe, the Archive bucket must be in the Finland zone to get the best price. When creating them, they should look like this:

<figure>
  <a href="/assets/img/2022-06-12/buckets.png"><img src="/assets/img/2022-06-10/buckets.png"></a>
  <figcaption></figcaption>
</figure>

Inside the Standard bucket, we will create two folders, one called credentials and another one called executions. In the credentials folder we will put the credentials json file and the token in .pickle format that we obtained when executing the code locally. In the executions folder we will put the last_execution_date.txt file that contains the following string inside: **2022-06-06T02:08:28Z**. This date means the date from which any photos found in the Google Photos account will be downloaded and uploaded to the Archive bucket. This date inside the file is modified in each correct execution of the Google Cloud Function.
The next step is to create the Google Cloud Function to execute an adaptation of the code that we have already created locally.
To create the function we go to the Google Cloud Functions section on GCP and create a function. We define the initial parameters as shown below. Defining the function in the Python language.

<figure>
  <a href="/assets/img/2022-06-12/gcf1.png"><img src="/assets/img/2022-06-10/gcf1.png"></a>
  <figcaption></figcaption>
</figure>

<figure>
  <a href="/assets/img/2022-06-12/gcf2.png"><img src="/assets/img/2022-06-10/gcf2.png"></a>
  <figcaption></figcaption>
</figure>

In the function we will create a file that we will call Google.py and we will paste the code of our file with the same name.

<figure>
  <a href="/assets/img/2022-06-12/gcf3.png"><img src="/assets/img/2022-06-10/gcf3.png"></a>
  <figcaption></figcaption>
</figure>

In the requirements.txt file we will put the following libraries to have them available when all the code is executed.

```
requests==2.27.1
google-auth-oauthlib==0.5.1
google-api-python-client==2.50.0
google-auth-httplib2==0.1.0
google-auth-oauthlib==0.5.1
google-cloud-storage
```

And finally, in the main.py file, we paste this new code adapted to interact with both the API and the corresponding buckets.

```python
import os
import Google
import requests
import datetime
from google.cloud import storage


# Download credentials file from GCS
def download_binary_file_from_gcs(bucket_name, blob_path):
    file_name = blob_path.split('/')[-1]
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    file_content = bucket.blob(blob_path).download_to_filename('/tmp/' + file_name)
    print('Downloaded file to /tmp/' + file_name)


# Upload credentials file to GCS
def upload_file_to_gcs(bucket_name, blob_path, tmp_file_path):
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(blob_path)
    blob.upload_from_filename(tmp_file_path)
    print('Uploaded file to GCS: ' + bucket_name + '/' + blob_path)


def blob_exists_on_gcs(bucket_name, blob_path):
   client = storage.Client()
   bucket = client.get_bucket(bucket_name)
   blob = bucket.blob(blob_path)
   return blob.exists()


# Download file from Google Photos
def download_media_file(url, target_folder, file_name):
    response = requests.get(url)
    if response.status_code == 200:
        print(f'Downloading file {file_name}')
        with open(os.path.join(target_folder, file_name), 'wb') as f:
            f.write(response.content)
            f.close()
        print(f'Downloded file {file_name}')


# Run Cloud Function
def run(request):
    # Set variables
    credentials_bucket_name = 'album-352111-gd-to-gcs-tmp'
    execution_bucket_name = 'album-352111-gd-to-gcs-tmp'
    credentials_json_blob_path = 'credentials/credentials.json'
    credentials_json_tmp_path = '/tmp/credentials.json'
    album_bucket_name = 'album-roberto-gabriel-martinez-de-zayas'
    last_execution_date_blob_path = 'execution/last_execution_date.txt'
    last_execution_date_tmp_path = '/tmp/last_execution_date.txt'
    api_name = 'photoslibrary'
    api_version = 'v1'
    credentials_pickle_blob_path = f'credentials/token_{api_name}_{api_version}.pickle'
    credentials_pickle_tmp_path = f'/tmp/token_{api_name}_{api_version}.pickle'
    scopes = ['https://www.googleapis.com/auth/photoslibrary']


    # Download credentials and execution files
    download_binary_file_from_gcs(credentials_bucket_name, credentials_json_blob_path)
    download_binary_file_from_gcs(credentials_bucket_name, credentials_pickle_blob_path)
    download_binary_file_from_gcs(execution_bucket_name, last_execution_date_blob_path)


    # Load last execution date
    with open(last_execution_date_tmp_path, 'r') as f:
        last_execution_date = f.readline()
        f.close()


    # Connect to Google Photos and get media items
    service = Google.Create_Service(credentials_bucket_name, credentials_json_tmp_path, credentials_pickle_blob_path, api_name, api_version, credentials_pickle_tmp_path, scopes)
    mymediaItems = service.mediaItems().list(pageSize=50).execute()


    # Download files and upload to GCS
    for mediaItem in mymediaItems['mediaItems']:
        if mediaItem['mediaMetadata']['creationTime'] >= last_execution_date:
            new_mediaItem_blob_path = datetime.datetime.strptime(mediaItem['mediaMetadata']['creationTime'], '%Y-%m-%dT%H:%M:%SZ').strftime('%Y%m') + '/' + mediaItem['filename']
            if not blob_exists_on_gcs(album_bucket_name, new_mediaItem_blob_path):
                download_media_file(mediaItem['baseUrl'], '/tmp/', mediaItem['filename'])
                upload_file_to_gcs(album_bucket_name, new_mediaItem_blob_path, '/tmp/' + mediaItem['filename'])
            else:
                print(f'File {new_mediaItem_blob_path} already exists on bucket {album_bucket_name}.')


    # Update last execution date file
    now = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
    with open(last_execution_date_tmp_path, 'w') as f:
        last_execution_date = f.write(now)
        f.close()
    upload_file_to_gcs(credentials_bucket_name, last_execution_date_blob_path, last_execution_date_tmp_path)
    
    print(f'Function finished with last execution date: {now}')
```

Remember to change the entry point to “run”. Then we deploy the function.
Once the function has been deployed, we can test it in the testing section. When executing it, we must see in the log how the files with a creation date after the date indicated in the last_execution_date.txt file are downloaded. If the token is no longer valid, we will see that it is updated again. We can also verify that the download went well if we see the files in the Archive bucket, in the folder with the year and month in which the file was originally created, since it dynamically creates the destination path.

<figure>
  <a href="/assets/img/2022-06-12/uploaded_photos_to_gcs.png"><img src="/assets/img/2022-06-10/uploaded_photos_to_gcs.png"></a>
  <figcaption></figcaption>
</figure>

It would only be necessary to create an alarm that executes the function once a month. To do this, we look for the Cloud Scheduler service in GCP and create a new Job. We configure it to run once a month, for example on the 1st of each month, and pass it the activation url of the function.

<figure>
  <a href="/assets/img/2022-06-12/scheduler1.png"><img src="/assets/img/2022-06-10/scheduler1.png"></a>
  <figcaption></figcaption>
</figure>

The rest of the parameters can be configured to your own liking. Setting number of retries in case of failure etc. Once the job is created, we test that it executes the function by forcing an execution (in the three points on the right Actions and then in Force a job run).
We verify in the Loggin service that our flow executed correctly.
And this would be it. Now we have an automated process that allows us to upload all the photos that we take with the mobile to a bucket in Google Cloud Platform. At a fairly low price compared to other solutions, ad yes, with some restrictions that must be taken into account as the cost of consuming that info too frequently.
This has been a project mainly as a contact with some of the basic services of Google Cloud Platform and to give a solution to a real problem (of very little importance).
I hope you liked it.
Go great!